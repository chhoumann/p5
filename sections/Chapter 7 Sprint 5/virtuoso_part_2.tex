\section{Implementing Virtuoso}
Following the user stories laid out in section \ref{section:sprintPlanningSprint5}, we will look at implementing a database that supports RDF data, as well as SPARQL queries.

The previous group responsible for \knox{} databases implemented a Apache Jena Fuseki database, as described in section \ref{currentState}.
This implementation was, however, not suitable for \knox{}, as it needed to be restarted periodically to 'save' the data. This was due to their usage of the HDT format for storing the data\cite{knox2020}.

For this reason, we decided to invest time in implementing OpenLink Virtuoso\cite{OpenLinkSoftwareVirtuoso} for the \knox{} database layer. We had already made this decision in sprint 2, as seen in chapter \ref{chapter:sprint2}.

Before starting the implementation, we decided to examine whether Virtuoso should still be our choice.
We attempted to gather feedback from the other groups in the \knox{} project, but they had very few requirements for the RDF database. All they wanted to do was store and retrieve RDF data.
They did ask for this to be facilitated through SPARQL queries to a given endpoint, and that they could insert data in Turtle format. Our implementation of those specifications will be discussed shortly.

Virtuoso supports both, which made it a good choice for \knox{} --- specification-wise. We also did some research on the performance of Virtuoso, and it seemed to do fine in various benchmarks\cite{addleseeComparingLinkedData2019}\cite{jovanovikBenchmarkingVirtuosoMighty2018}.

Another potential choice could be Linked Data Fragments (LDF). However, LDF seems to use HDT, which would not solve the issue of restarting the database periodically. Therefore, Virtuoso remains our choice.

When we first looked at Virtuoso, we found some frameworks that would help us facilitate data insertion and retrieval.  However, during implementation, none of these frameworks proved useful. Multiple frameworks, like \textit{dotNetRDF} (C\#) and \textit{RDFLib} (Python), were attempted to be used, in order to reduce complexity of the implementation. However, we were unable to establish a connection to the database with any of the frameworks, so the efforts were abandoned. Instead, we decided to use the Virtuoso SPARQL endpoint directly, which worked well for our needs.

As for our implementation, we did not choose to publish the SPARQL endpoint for \knox{} usage directly. Instead, we set up a web API in C\#.
Our reason for doing this is to encapsulate database access, such that we can change the database in the future without affecting the API.
This API is connected to Virtuoso, and both run in a dockerized environment. They run on the same network, so that they can communicate with each other.

As mentioned, Virtuoso supports insertion of data in Turtle format, and retrieval of data in RDF/XML format. Inserting and querying data is done through the Virtuoso SPARQL endpoint. The C\# API we have set up relies on this endpoint to do the actual work. We pass on the queries and data, acting as a proxy between the user and the Virtuoso endpoint. At least, this is the case when users send SPARQL queries to the API. When users send Turtle data to the API, we wrap it in a SPARQL query with some preset parameters, and then send it to Virtuoso. For example, we have specified a default graph, in case the user does not specify one.
This can be seen in the \texttt{Insert} function, shown in code snippet \ref{lst:virtuoso_data_insertion}. Users send a HTTP POST request to the API, with the data in Turtle format as a string, along with a string denoting the graph name they want to insert the data to. The API then sends the data to Virtuoso, which can be seen in \texttt{InsertTurtleGraph}, also shown in code snippet \ref{lst:virtuoso_data_insertion}. As can be seen in the definition for \texttt{InsertTurtleGraph}, we have specified a default graph. First, we parse the Turtle data from the input string, so we are certain that it is valid Turtle. Then, we log how many triples we have found in the given graph, after which we chunk and send the data to Virtuoso. We will return to the chunking logic momentarily, but for now, know that it returns the Turtle data as an array of strings, each of which is a chunk of data. Each chunk is wrapped in a SPARQL query that specifies that data should be inserted into the given graph. This query is then sent to the Virtuoso endpoint through HTTP.

\begin{lstlisting}[language=CSharp, caption={Data insertion endpoint and data insertion logic}, label={lst:virtuoso_data_insertion}]
[HttpPost, Route("/[controller]/")]
public async Task<IActionResult> Insert(string turtle, string? graph)
{
    string? virtuosoEndpoint = Environment.GetEnvironmentVariable("VIRTUOSO_ENDPOINT");
    if (virtuosoEndpoint == null)
    {
        return StatusCode((int)HttpStatusCode.InternalServerError, "VIRTUOSO_ENDPOINT environment variable not set");
    }

    try
    {
        string insertResponse = !string.IsNullOrEmpty(graph) ?
            await new VirtuosoDataStore(virtuosoEndpoint).InsertTurtleGraph(turtle, graph) :
            await new VirtuosoDataStore(virtuosoEndpoint).InsertTurtleGraph(turtle);

        return Ok(insertResponse);
    }
    catch (Exception e)
    {
        return StatusCode(503, e.Message);
    }
}

public async Task<string> InsertTurtleGraph(string turtle, string? graphName = "knox")
{
    IGraph g = new Graph()
    {
        BaseUri = uri,
    };
    
    GraphHandler graphHandler = new(g);
    TurtleParser parser = new();
    parser.Load(graphHandler, new StringReader(turtle));
    
    Console.WriteLine($"Triples in given graph: {g.Triples.Count}");

    string[] chunkedQueries = ChunkRecords(turtle, "\n\n", 50);

    foreach (string chunkedQuery in chunkedQueries)
    {
        StringBuilder sb = new();
        
        sb.Append("INSERT DATA { GRAPH <" + graphName + "> {");
        sb.Append(chunkedQuery);
        sb.Append("} }");

        await Query(sb.ToString());
    }

    return "OK";
}
\end{lstlisting}


We have also implemented chunking logic for data insertion, such that users do not get an error when sending a large amount of data. This was done because we, during our tests, experienced that Virtuoso would error out when sending a large amount of data --- it could not execute such large amounts of generated SQL code. The logic for diving chunks of data into smaller chunks is implemented in the \texttt{ChunkRecords} function, which can be seen in code snippet \ref{lst:virtuoso_chunking}. The logic itself is to divide a large string of input data into smaller chunks given some separator and chunk size. This is done by first splitting the string into an array of strings with the separator, and then combining individual strings into chunks with the given size.

\begin{lstlisting}[language=CSharp, caption={Chunking logic for large queries}, label={lst:virtuoso_chunking}]
private string[] ChunkRecords(string input, string separator, int chunkSize = 10)
{
    List<string> chunks = new();
    string[] strings = input.Split(separator);
    
    for (int i = 0; i < strings.Length / chunkSize; i++)
    {
        string[] chunk = strings.Skip(i * chunkSize).Take(chunkSize).ToArray();
        chunks.Add(string.Join(separator, chunk));
    }

    int remainder = strings.Length % chunkSize;
    int taken = chunks.Count * chunkSize;

    string[] lastChunk = strings.Skip(taken).Take(remainder).ToArray();
    chunks.Add(string.Join(separator, lastChunk));
    
    return chunks.ToArray();
}
\end{lstlisting}


The architecture for this solution can be seen in figure \ref{fig:virtuoso_architecture}. The users do not communicate with Docker, as the figure may imply, but rather, Docker allows for communication between the users and the containers within. Users access the exposed services through HTTP requests. The Docker containers are configured to run on the same network, so that they can communicate with each other.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{Images/Virtuoso_Architecture.png}
\caption{Architecture for Virtuoso API}
\label{fig:virtuoso_architecture}
\end{figure}

We have not had time to deploy the API to the production environment. Therefore, deployment will be pushed to the next sprint.